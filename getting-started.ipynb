{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load dependencies\n",
    "\n",
    "The following loads the necessary dependencies and checks the Python version (at runtime). ML-Agents Toolkit (v0.3 onwards) requires Python 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.7.6 (default, Jan  8 2020, 20:23:39) [MSC v.1916 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "from mlagents_envs.side_channel.engine_configuration_channel import EngineConfig, EngineConfigurationChannel\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Start the environment\n",
    "`UnityEnvironment` launches and begins communication with the environment when instantiated.\n",
    "\n",
    "Environments contain _brains_ which are responsible for deciding the actions of their associated _agents_. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine_configuration_channel = EngineConfigurationChannel()\n",
    "env_name=\"PoliceInt.exe\"\n",
    "env = UnityEnvironment(file_name=env_name, side_channels = [engine_configuration_channel])\n",
    "\n",
    "#Reset the environment\n",
    "env.reset()\n",
    "\n",
    "# Set the default brain to work with\n",
    "group_name = env.get_agent_groups()[0]\n",
    "group_spec = env.get_agent_group_spec(group_name)\n",
    "\n",
    "# Set the time scale of the engine\n",
    "engine_configuration_channel.set_configuration_parameters(time_scale = 3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the observation and state spaces\n",
    "We can reset the environment to be provided with an initial set of observations and states for all the agents within the environment. In ML-Agents, _states_ refer to a vector of variables corresponding to relevant aspects of the environment for an agent. Likewise, _observations_ refer to a set of relevant pixel-wise visuals for an agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations :  3\n",
      "Is there a visual observation ? True\n",
      "Agent visual observation looks like:\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Get the state of the agents\n",
    "step_result = env.get_step_result(group_name)\n",
    "\n",
    "# Examine the number of observations per Agent\n",
    "print(\"Number of observations : \", len(group_spec.observation_shapes))\n",
    "\n",
    "# Is there a visual observation ?\n",
    "vis_obs = any([len(shape) == 3 for shape in group_spec.observation_shapes])\n",
    "print(\"Is there a visual observation ?\", vis_obs)\n",
    "\n",
    "# Examine the visual observations\n",
    "if vis_obs:\n",
    "    vis_obs_index = next(i for i,v in enumerate(group_spec.observation_shapes) if len(v) == 3)\n",
    "    print(\"Agent visual observation looks like:\")\n",
    "    obs = step_result.obs[vis_obs_index]\n",
    "    plt.imshow(obs[0,:,:,:])\n",
    "else:\n",
    "    # Examine the state space for the first observation for the first agent\n",
    "    print(\"First Agent observation looks like: \\n{}\".format(step_result.obs[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Take random actions in the environment\n",
    "Once we restart an environment, we can step the environment forward and provide actions to all of the agents within the environment. Here we simply choose random actions based on the `action_space_type` of the default brain.\n",
    "\n",
    "Once this cell is executed, 10 messages will be printed that detail how much reward will be accumulated for the next 10 episodes. The Unity environment will then pause, waiting for further signals telling it what to do next. Thus, not seeing any animation is expected when running this cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: -10.379999697208405 Episode 0\n",
      "Total reward this episode: -10.939999863505363 Episode 1\n",
      "Total reward this episode: -18.97999895364046 Episode 2\n",
      "Total reward this episode: -10.939999863505363 Episode 3\n",
      "Total reward this episode: -12.879999421536922 Episode 4\n",
      "Total reward this episode: -11.169999577105045 Episode 5\n",
      "Total reward this episode: -11.479999773204327 Episode 6\n",
      "Total reward this episode: -0.029999881982803345 Episode 7\n",
      "Total reward this episode: -13.869999282062054 Episode 8\n",
      "Total reward this episode: -14.619999200105667 Episode 9\n",
      "Total reward this episode: -6.839999169111252 Episode 10\n",
      "Total reward this episode: -1.0099997743964195 Episode 11\n",
      "Total reward this episode: -10.939999863505363 Episode 12\n",
      "Total reward this episode: -10.379999697208405 Episode 13\n",
      "Total reward this episode: -14.009999990463257 Episode 14\n",
      "Total reward this episode: -10.379999697208405 Episode 15\n",
      "Total reward this episode: -10.379999697208405 Episode 16\n",
      "Total reward this episode: -10.939999863505363 Episode 17\n",
      "Total reward this episode: -10.379999697208405 Episode 18\n",
      "Total reward this episode: -10.939999863505363 Episode 19\n",
      "Total reward this episode: -10.379999697208405 Episode 20\n",
      "Total reward this episode: -10.939999863505363 Episode 21\n",
      "Total reward this episode: -10.939999863505363 Episode 22\n"
     ]
    },
    {
     "ename": "UnityTimeOutException",
     "evalue": "The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Agents are linked to the appropriate Brains\n\t The environment and the Python interface have compatible versions.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnityTimeOutException\u001b[0m                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-9bb1241fcbda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumn_stack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbranch_size\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn_agents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbranch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_actions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m         \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m         \u001b[0mstep_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_step_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgroup_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mepisode_rewards\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mstep_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UnityMLAgents\\lib\\site-packages\\mlagents_envs\\timers.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__qualname__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UnityMLAgents\\lib\\site-packages\\mlagents_envs\\environment.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[0mstep_input\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_generate_step_input\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_env_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mhierarchical_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"communicator.exchange\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexchange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutputs\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mUnityCommunicationException\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Communicator has stopped.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UnityMLAgents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mexchange\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mmessage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_input\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCopyFrom\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 112\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll_for_timeout\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    113\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\UnityMLAgents\\lib\\site-packages\\mlagents_envs\\rpc_communicator.py\u001b[0m in \u001b[0;36mpoll_for_timeout\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munity_to_external\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparent_conn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout_wait\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m             raise UnityTimeOutException(\n\u001b[1;32m---> 91\u001b[1;33m                 \u001b[1;34m\"The Unity environment took too long to respond. Make sure that :\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m                 \u001b[1;34m\"\\t The environment does not need user interaction to launch\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m                 \u001b[1;34m\"\\t The Agents are linked to the appropriate Brains\\n\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnityTimeOutException\u001b[0m: The Unity environment took too long to respond. Make sure that :\n\t The environment does not need user interaction to launch\n\t The Agents are linked to the appropriate Brains\n\t The environment and the Python interface have compatible versions."
     ]
    }
   ],
   "source": [
    "for episode in range(1000000):\n",
    "    env.reset()\n",
    "    step_result = env.get_step_result(group_name)\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        action_size = group_spec.action_size\n",
    "        if group_spec.is_action_continuous():\n",
    "            action = np.random.randn(step_result.n_agents(), group_spec.action_size)\n",
    "            \n",
    "        if group_spec.is_action_discrete():\n",
    "            branch_size = group_spec.discrete_action_branches\n",
    "            action = np.column_stack([np.random.randint(0, branch_size[i], size=(step_result.n_agents())) for i in range(len(branch_size))])\n",
    "        env.set_actions(group_name, action)\n",
    "        env.step()\n",
    "        step_result = env.get_step_result(group_name)\n",
    "        episode_rewards += step_result.reward[0]\n",
    "        done = step_result.done[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards)+\" Episode \"+ str(episode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Close the environment when finished\n",
    "When we are finished using an environment, we can close it with the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": [
     "\n",
     "#%env_name = \"PoliceInt.exe\"  # Name of the Unity environment binary to launch\n"
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
